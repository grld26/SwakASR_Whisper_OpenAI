{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nyaeb1UFerq",
        "outputId": "82eaa843-d741-4098-84a5-b30e5d1c6b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git -q\n",
        "! pip install gradio -q\n",
        "! pip install jiwer -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import time\n",
        "# language = \"Malay\" \n",
        "import gradio as gr\n",
        "from jiwer import wer"
      ],
      "metadata": {
        "id": "97HUjR6AFl8s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transcribe the speech\n",
        "def transcribe1(micRecord, audioFileUpload, model_size):\n",
        "\n",
        "    # source = upload if upload is not None else micRecord\n",
        "    # model = whisper.load_model(model_size)\n",
        "    # # mel = whisper.log_mel_spectrogram(source).to(model.device)\n",
        "    # result = model.transcribe(source)\n",
        "    # global hypothesis\n",
        "    # hypothesis = result[\"text\"]\n",
        "\n",
        "    source = audioFileUpload if audioFileUpload is not None else micRecord\n",
        "    model = whisper.load_model(model_size)\n",
        "    source = whisper.load_audio(source)\n",
        "    source = whisper.pad_or_trim(source)\n",
        "    mel = whisper.log_mel_spectrogram(source).to(model.device)\n",
        "    options = whisper.DecodingOptions(fp16=False)\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    global hypothesis\n",
        "    hypothesis = result.text\n",
        " \n",
        "    return {\n",
        "        output_transcript: hypothesis,\n",
        "        hiddenRow2: gr.update(visible=True)\n",
        "    }\n",
        "        \n",
        "# Function to calculate the Word Error Rate\n",
        "def calculate_WER(reference):\n",
        "\n",
        "    error = wer(reference, hypothesis) * 100\n",
        "\n",
        "    return {\n",
        "        output_wer: error\n",
        "    }\n",
        "\n",
        "# Function to display the visibility of the WER calculation section\n",
        "def vis_calculate_WER():\n",
        "    return gr.update(visible=True)\n",
        "\n",
        "# Function to conceal conclusion\n",
        "def vis_hide_concl():\n",
        "    return gr.update(visible=False)\n",
        "\n",
        "# Function to display the conclusion\n",
        "def vis_conclude():\n",
        "    return gr.update(visible=True)\n",
        "\n",
        "# Function to conceal WER calculation\n",
        "def vis_hide_calc():\n",
        "    return gr.update(visible=False)\n",
        "\n",
        "with gr.Blocks() as systemOverview:\n",
        "\n",
        "  gr.Markdown(\n",
        "        \"\"\"# Benchmarking Whisper OpenAI For Sarawak Languages\n",
        "        <br> The Whisper model that is based on the end-to-end (E2E), encoder-decoder transformer has the potential to serve as a very effective model in facilitating rapid breakthroughs in automatic speech recognition (ASR) for under-resourced languages of Sarawak, namely the Sarawak Malay, Iban, Melanau, and the Bidayuh dialects of Jagoi and Bukar Sadong. This developed system integrates the aforementioned model, Whisper, and the JIWER package with the aim of measuring the recognition accuracy of Whisper on under-resourced Sarawak languages. The results may then serve as a benchmark to indicate the recent advances in ASR for under-resourced Sarawak languages.   \n",
        "        \n",
        "        \n",
        "        <br> Fundamentally, there are a number of models for Whisper that correspond to the parameter size namely the tiny, base, small, medium, and large models. They are distinguished according to the size of parameters that are 39 million, 74 million, 244 million, 769 million and 1.55 billion respectively.\n",
        "        \n",
        "        \n",
        "        <br> The accuracy of Whisper for Sarawak languages is evaluated based on the Word Error Rate (WER). This evaluation metric consists of word substitutions (S), deletions (D), insertions (I), and number of words (N), where it is calculated by summing the word substitutions, deletions, and insertions together, subsequently dividing the result by the number of words. In essence, the WER cannot be a negative number, but it could be above 100% in instances where there exist more errors than words in the reference. The formula is as follows: \n",
        "        \n",
        "        WER = (S + D + I) / N\n",
        "\n",
        "    \n",
        "        <br> Created by Gerald E.\n",
        "        \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "#Audio Recording \n",
        "with gr.Blocks() as audioRec:\n",
        "\n",
        "  gr.Markdown(\n",
        "        \"\"\"# Benchmarking Whisper OpenAI For Sarawak Languages\n",
        "        <br> User Guide\n",
        "        1. Select a Whisper model from the dropdown options according to the parameter size.\n",
        "        2. Upload an audio file (M4A, MP3, MP4, MPEG, MPGA, WAV and WEBM) OR record your voice on your microphone.\n",
        "        3. Hit the \"Transcribe\" button to prompt Whisper to transcribe the speech recording.\n",
        "        4. You may also calculate the WER for an innacurate transcription upon obtaining Whisper's transcription by hitting the \"Inaccurate Transcription, Calculate WER\" button.\n",
        "        5. Enter the reference text transcript, then hit the \"Calculate\" button.        \n",
        "        \"\"\"\n",
        "  )\n",
        "  \n",
        "  input_model_type = gr.Dropdown(label=\"Select The Whisper Model According To Parameter Size\",value=\"Please Select A Model\",choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n",
        "  input_audio1 = gr.Audio(source=\"upload\", type=\"filepath\", label=\"Upload Audio File\")\n",
        "  gr.Markdown(\"OR\")\n",
        "  input_audio2 = gr.Audio(label=\"Record Your Voice On Your Microphone\",source=\"microphone\", type=\"filepath\")\n",
        "  btn_submit2  = gr.Button(\"Transcribe\")\n",
        "  output_transcript = gr.Textbox(type=\"text\", label=\"Whisper's Transcription Result\")\n",
        "\n",
        "  with gr.Column(visible=False) as hiddenRow2:\n",
        "      gr.Markdown(\"Is Whisper's transcription accurate?\")\n",
        "      with gr.Row() as selectionBtn:\n",
        "        btn_submit3 = gr.Button(\"Accurate Transcription\")\n",
        "        btn_submit4 = gr.Button(\"Inaccurate Transcription, Calculate WER\")\n",
        "\n",
        "  with gr.Column(visible=False) as hiddenRow3: \n",
        "      input_ref = gr.Textbox(label=\"Reference Text Transcript\", placeholder=\"Please Enter The Reference Text Transcript\")\n",
        "      btn_submit5 = gr.Button(\"Calculate\")\n",
        "      output_wer = gr.Textbox(label=\"Word Error Rate (%)\")\n",
        "\n",
        "  with gr.Row(visible=False) as hiddenRowConclude: \n",
        "      gr.Markdown(\"Great! Thank you for using this transcriber, be seeing you.\")\n",
        "\n",
        "  btn_submit2.click(\n",
        "        transcribe1,\n",
        "        [input_audio1, input_audio2, input_model_type],\n",
        "        [output_transcript, hiddenRow2]\n",
        "    )\n",
        "\n",
        "  btn_submit3.click(\n",
        "        vis_conclude,\n",
        "        [],\n",
        "        hiddenRowConclude\n",
        "    )\n",
        "\n",
        "  btn_submit3.click(\n",
        "        vis_hide_calc,\n",
        "        [],\n",
        "        hiddenRow3\n",
        "    )\n",
        "\n",
        "  btn_submit4.click(\n",
        "        vis_calculate_WER,\n",
        "        [],\n",
        "        hiddenRow3\n",
        "    )\n",
        "  \n",
        "  btn_submit4.click(\n",
        "        vis_hide_concl,\n",
        "        [],\n",
        "        hiddenRowConclude\n",
        "    )\n",
        "\n",
        "  btn_submit5.click(\n",
        "        calculate_WER,\n",
        "        input_ref,\n",
        "        output_wer\n",
        "    )\n",
        "\n",
        "\n",
        "demo1 = gr.TabbedInterface([systemOverview, audioRec], [\"Overview\",\"Transcriber\"])\n",
        "\n",
        "demo1.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "Gctrmup3Foez",
        "outputId": "e06edd66-878b-4b43-ca9f-0ec8e923c081"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2c1a94584b8a44fe55.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2c1a94584b8a44fe55.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}